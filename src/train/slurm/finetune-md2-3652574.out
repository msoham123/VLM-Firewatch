---------------------------------------
Begin Slurm Prolog: Nov-25-2025 15:02:26
Job ID:    3652574
User ID:   smanoli3
Account:   coc
Job name:  finetune_md2
Partition: coe-gpu
QOS:       coe-ice
---------------------------------------
INFO:__main__:Using device: cuda, dtype: torch.float16
INFO:__main__:Loading model: vikhyatk/moondream2
PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Traceback (most recent call last):
  File "/home/hice1/smanoli3/VLM-FireWatch/src/train/finetune_moondream2.py", line 570, in <module>
    main()
  File "/home/hice1/smanoli3/VLM-FireWatch/src/train/finetune_moondream2.py", line 507, in main
    fine_tuner = Moondream2FineTuner(device=device, dtype=dtype)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hice1/smanoli3/VLM-FireWatch/src/train/finetune_moondream2.py", line 199, in __init__
    self._load_model_and_tokenizer()
  File "/home/hice1/smanoli3/VLM-FireWatch/src/train/finetune_moondream2.py", line 225, in _load_model_and_tokenizer
    self.model = self.model.to(self.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4343, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/hice1/smanoli3/scratch/conda/conda_envs/vlm_firewatch_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

---------------------------------------
Begin Slurm Epilog: Nov-25-2025 15:03:54
Job ID:        3652574
User ID:       smanoli3
Account:       coc
Job name:      finetune_md2
Resources:     cpu=2,gres/gpu:h100=1,mem=128G,node=1
Rsrc Used:     cput=00:02:58,vmem=0,walltime=00:01:29,mem=1809272K,energy_used=0
Partition:     coe-gpu
QOS:           coe-ice
Nodes:         atl1-1-03-012-3-0
---------------------------------------
